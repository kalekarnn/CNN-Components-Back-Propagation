# CNN-Components-Back-Propagation


## Model Summary 

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 26, 26]             832
              ReLU-2           [-1, 32, 26, 26]               0
       BatchNorm2d-3           [-1, 32, 26, 26]              64
           Dropout-4           [-1, 32, 26, 26]               0
            Conv2d-5           [-1, 24, 26, 26]           6,936
              ReLU-6           [-1, 24, 26, 26]               0
       BatchNorm2d-7           [-1, 24, 26, 26]              48
           Dropout-8           [-1, 24, 26, 26]               0
            Conv2d-9           [-1, 16, 13, 13]           3,472
             ReLU-10           [-1, 16, 13, 13]               0
      BatchNorm2d-11           [-1, 16, 13, 13]              32
          Dropout-12           [-1, 16, 13, 13]               0
           Conv2d-13             [-1, 24, 7, 7]           3,480
             ReLU-14             [-1, 24, 7, 7]               0
      BatchNorm2d-15             [-1, 24, 7, 7]              48
          Dropout-16             [-1, 24, 7, 7]               0
           Conv2d-17             [-1, 16, 7, 7]           3,472
             ReLU-18             [-1, 16, 7, 7]               0
      BatchNorm2d-19             [-1, 16, 7, 7]              32
           Conv2d-20             [-1, 10, 5, 5]           1,450
        AvgPool2d-21             [-1, 10, 1, 1]               0
================================================================
Total params: 19,866
Trainable params: 19,866
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.29
Params size (MB): 0.08
Estimated Total Size (MB): 1.37
----------------------------------------------------------------

```

## Training Logs

```
loss=0.12988252937793732 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.43it/s]
Test set: Average loss: 0.0732, Accuracy: 9783/10000 (97.83%)

loss=0.06300240755081177 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.92it/s]
Test set: Average loss: 0.0549, Accuracy: 9826/10000 (98.26%)

loss=0.060625944286584854 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.80it/s]
Test set: Average loss: 0.0435, Accuracy: 9870/10000 (98.70%)

loss=0.03916307911276817 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.54it/s]
Test set: Average loss: 0.0367, Accuracy: 9879/10000 (98.79%)

loss=0.05059107765555382 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.60it/s]
Test set: Average loss: 0.0328, Accuracy: 9897/10000 (98.97%)

loss=0.04774579033255577 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.99it/s]
Test set: Average loss: 0.0335, Accuracy: 9886/10000 (98.86%)

loss=0.02128792740404606 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.15it/s]
Test set: Average loss: 0.0270, Accuracy: 9900/10000 (99.00%)

loss=0.02550916187465191 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.46it/s]
Test set: Average loss: 0.0231, Accuracy: 9926/10000 (99.26%)

loss=0.049538303166627884 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.08it/s]
Test set: Average loss: 0.0287, Accuracy: 9911/10000 (99.11%)

loss=0.040807243436574936 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.59it/s]
Test set: Average loss: 0.0250, Accuracy: 9922/10000 (99.22%)

loss=0.004177652299404144 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.93it/s]
Test set: Average loss: 0.0215, Accuracy: 9921/10000 (99.21%)

loss=0.013583888299763203 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.96it/s]
Test set: Average loss: 0.0227, Accuracy: 9917/10000 (99.17%)

loss=0.005426796153187752 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.74it/s]
Test set: Average loss: 0.0214, Accuracy: 9928/10000 (99.28%)

loss=0.00636220583692193 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.85it/s]
Test set: Average loss: 0.0212, Accuracy: 9922/10000 (99.22%)

loss=0.02467947266995907 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.78it/s]
Test set: Average loss: 0.0208, Accuracy: 9937/10000 (99.37%)

loss=0.00425241282209754 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 27.02it/s]
Test set: Average loss: 0.0187, Accuracy: 9933/10000 (99.33%)

loss=0.015966152772307396 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.76it/s]
Test set: Average loss: 0.0190, Accuracy: 9939/10000 (99.39%)

loss=0.01784677617251873 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.70it/s]
Test set: Average loss: 0.0215, Accuracy: 9928/10000 (99.28%)

loss=0.05131326615810394 batch_id=468: 100%|██████████| 469/469 [00:17<00:00, 26.21it/s]
Test set: Average loss: 0.0182, Accuracy: 9929/10000 (99.29%)

loss=0.041904449462890625 batch_id=468: 100%|██████████| 469/469 [00:18<00:00, 25.62it/s]
Test set: Average loss: 0.0199, Accuracy: 9940/10000 (99.40%)
```
